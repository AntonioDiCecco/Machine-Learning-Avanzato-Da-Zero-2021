{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOAI Lezione patica K Nearest Neighbors\n",
    "\n",
    "Questo è un semplice progetto per mettere in pratica la teorica e il funzionamento dell'algoritmo. Durante questa esercitazione utilizzeremo funzioni e metodi nuovi che semplificheranno i concetti di calcolo.\n",
    "I punti base di questa esercitazione sono:\n",
    "1. import delle librerie\n",
    "2. normalizzazione dei dati\n",
    "3. applicazione dell'algoritmo\n",
    "4. valutazione dei risultati \n",
    "5. miglioramento dell'algoritmo\n",
    "\n",
    "\n",
    "\n",
    "### Import delle librerie\n",
    "Le classiche librerie da importare sono: pandas, numpy, seaborn e matplotlib ma non dimetichiamoci di inserire %matplotlib inline per vedere direttamente sul notebook i grafici che andremo a creare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leggiamo il dataset da analizzare\n",
    "Utilizziamo la funzione .read_scv() di pandas scrivendo il nome del file che in questo caso si trova nella stessa cartella in cui è contenuto il notebook, se fosse stato in un'altra cartella avrei dovuto far precedere al nome del file il percorso assoluto del file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SOAI-KNN_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Controllimamo come sono questi dati**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizziamo i dati delle variabili\n",
    "\n",
    "Per rendere omogenei tra le varibili le distanze, rispettando le singole differenze dobbiamo normalizzare i dati standardizzando i valori rispetto ai parametri descrittivi di ciascuna variabile.\n",
    "\n",
    "Per fare questo utilizziamo una funzione di Scikit learn chiamata StandardScaler, creaiamo un'istanza e gli facciamo analizzare i dati tramite il metodo .fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.drop('TARGET',axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ora dobbiamo \"trasformare\" lo scaler con il metodo .transform() per ottenere un array come quello di partenza ma con i valori standardizzati**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "variabili_st = scaler.transform(df.drop('TARGET',axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora non ci resta che ricreare il nuovo dataset con le variabili standardizzate.\n",
    "**Come si fa?**\n",
    "Un datataset in pandas può essere creato, senza importare csv, semplicemnte con 2 parametri: l'elenco degli array di variabili  *(nel nostro caso* variabili _st *)* e l'elenco dei nomi di ciascuna variabile *(che nel nostro caso può essere preso da  df.columns)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = pd.DataFrame(variabili_st, columns=df.columns[:-1])\n",
    "#df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividiamo i dati in due parti (Train Test Split)\n",
    "\n",
    "**Utilizziamo una funzione di sklern per dividere il dataset in 2 parti una di train e una di test attraverso la funzione  train_test_split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le due righe sottostanti sono identiche la differenza è che nella prima si crea automaticamente un dataset mentre nel secondo si assegna un dataset esistente creato in precedenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(variabili_st, df['TARGET'], test_size=0.30, random_state=101)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_norm, df['TARGET'], test_size=0.30, random_state=101)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usiamo finalmente il KNN\n",
    "\n",
    "**Per prima cosa dobbiamo importarlo da sklern.neighbors (KNeighborsClassifier).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create un modello KNN e istanziamolo con il parametro K --> n_neighbors=1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyKnn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit del NOSTRO modello utilizzando il KNN con il seti di dati di training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyKnn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predizioni e Valutazioni del modello \n",
    "Ora vediamo come il nostro modello è in grado di prevedere nuovi dati che non ha mai visto prima (test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usiamo per questo il metodo predict dell'istanza del nostro modello fornendogli i dati di test_set e assegnamo il risultato ad una variabile.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = MyKnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non ci resta che creare la matrice di classificazione (confusion matrix) e il il classification report per vedere come il nostro modello riescea a prevedere questi dati.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIAMO SICURI CHE K=1 è la migliore scelta? proviamo altri K! \n",
    "Proviamo ad usare il metoto del gomito per trovare un k migliore!\n",
    "\n",
    "** Costruiamoci una variabile che possa contenere in una lista tutti i valori di errore che si commetterebbe utilizzando vari modelli KNN con differenti valori di K. Facciamo un grafico per visualizzare K ed errore in modo da visualizzare il miglior valore di K al quale riapplicheremo il modello di stima.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "errore = []\n",
    "\n",
    "# Facciamo un ciclo tante volte\n",
    "for i in range(1,40):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    errore.append(np.mean(pred_i != y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ora creiamo la figura  usando le informazioni del ciclo for appena fatto.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),errore, color='blue', linestyle='--', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Errore vs. Valore di K')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Errore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riaddestriamo il modell con il nuovo K (esempio k=30)\n",
    "\n",
    "**Riaddestriamo il modello con il k scelto e riclassifichiamo i dati per valurarne il nuovo adattamento attraverso il  classification report e la matrice di distribuzione.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW WITH K=30\n",
    "knn = KNeighborsClassifier(n_neighbors=30)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print('Valore di K=30')\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
